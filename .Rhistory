ggplot(data = ., aes(x = log.lambda, y = avg.fold.mse)) +
geom_vline(xintercept = min.lambda) +
geom_line(color = "red") +
geom_errorbar(aes(ymin = avg.fold.mse - avg.fold.se, ymax = avg.fold.mse + avg.fold.se), color = "gray50") +
labs(
title = "Average Test Fold MSE as a function of lambda",
x = "log(lambda)",
y = "Average Test MSE"
) +
theme(plot.title = element_text(hjust = 0.5))
# Function to do the cross-validation
lambda.cv = function(lambdas, start, data, resp, k = 5) {
### Parameters:
# lambdas : the sequence of lambdas that you want to create solutions for
# start : the starting beta coefficients
# data : the data to estimate the coefficients from
# resp : the response variable you're trying to predict
# returns a matrix of average test MSES against a sequence of given lambdas
folds = crossv_kfold(data, k = k)
path = NULL
i = 0
for (l in lambdas) {
# Reset the storage of the fold MSEs
fold.mses = NULL
fold.ses = NULL
i = i + 1
for (k in 1:nrow(folds)) {
# Grab the specific training indexes
train.idx = folds[k,1][[1]][[toString(k)]]$idx
test.idx = folds[k,2][[1]][[toString(k)]]$idx
# Split up the data into the training and test datasets
train.X = data[train.idx,]
test.X = data[test.idx,]
train.y = resp[train.idx]
test.y = resp[test.idx]
# Perform the logistic-LASSO
fit = LogLASSO.CD(X = train.X, y = train.y, beta = start, lambda = l)
u = as.matrix(test.X) %*% fit$coefficients
z = 1 / (1 + u)
# Calculate the test MSE for the fold
fold.mse = mean((test.y - z)^2)
fold.mses = c(fold.mses, fold.mse)
}
fold.mses = cbind(fold.mses, mean(fold.mses))
fold.ses = cbind(fold.ses, sqrt(var(fold.mses)/k))
path = rbind(path,
c(lambda = l, log.lambda = log(l),
avg.fold.mse = mean(fold.mses), avg.fold.se = mean(fold.ses)))
print(paste("Iteration:", i, "done"))
}
return(as.tibble(path))
}
cv.path = lambda.cv(lambda.seq, coeffs, X, y)
min.mse = min(cv.path$avg.fold.mse)
min.lambda = cv.path[which(cv.path$avg.fold.mse == min.mse),]$log.lambda
cv.path %>%
ggplot(data = ., aes(x = log.lambda, y = avg.fold.mse)) +
geom_vline(xintercept = min.lambda) +
geom_line(color = "red") +
geom_errorbar(aes(ymin = avg.fold.mse - avg.fold.se, ymax = avg.fold.mse + avg.fold.se), color = "gray50") +
labs(
title = "Average Test Fold MSE as a function of lambda",
x = "log(lambda)",
y = "Average Test MSE"
) +
theme(plot.title = element_text(hjust = 0.5))
# Function to do the cross-validation
lambda.cv = function(lambdas, start, data, resp, k = 5) {
### Parameters:
# lambdas : the sequence of lambdas that you want to create solutions for
# start : the starting beta coefficients
# data : the data to estimate the coefficients from
# resp : the response variable you're trying to predict
# returns a matrix of average test MSES against a sequence of given lambdas
folds = crossv_kfold(data, k = k)
path = NULL
i = 0
for (l in lambdas) {
# Reset the storage of the fold MSEs
fold.mses = NULL
fold.ses = NULL
i = i + 1
for (k in 1:nrow(folds)) {
# Grab the specific training indexes
train.idx = folds[k,1][[1]][[toString(k)]]$idx
test.idx = folds[k,2][[1]][[toString(k)]]$idx
# Split up the data into the training and test datasets
train.X = data[train.idx,]
test.X = data[test.idx,]
train.y = resp[train.idx]
test.y = resp[test.idx]
# Perform the logistic-LASSO
fit = LogLASSO.CD(X = train.X, y = train.y, beta = start, lambda = l)
u = exp(as.matrix(test.X) %*% fit$coefficients)
z = u / (1 + u)
# Calculate the test MSE for the fold
fold.mse = mean((test.y - z)^2)
fold.mses = c(fold.mses, fold.mse)
}
fold.mses = cbind(fold.mses, mean(fold.mses))
fold.ses = cbind(fold.ses, sqrt(var(fold.mses)/k))
path = rbind(path,
c(lambda = l, log.lambda = log(l),
avg.fold.mse = mean(fold.mses), avg.fold.se = mean(fold.ses)))
print(paste("Iteration:", i, "done"))
}
return(as.tibble(path))
}
cv.path = lambda.cv(lambda.seq, coeffs, X, y)
# Function to do the cross-validation
lambda.cv = function(lambdas, start, data, resp, k = 5) {
### Parameters:
# lambdas : the sequence of lambdas that you want to create solutions for
# start : the starting beta coefficients
# data : the data to estimate the coefficients from
# resp : the response variable you're trying to predict
# returns a matrix of average test MSES against a sequence of given lambdas
folds = crossv_kfold(data, k = k)
path = NULL
i = 0
for (l in lambdas) {
# Reset the storage of the fold MSEs
fold.mses = NULL
fold.ses = NULL
i = i + 1
for (k in 1:nrow(folds)) {
# Grab the specific training indexes
train.idx = folds[k,1][[1]][[toString(k)]]$idx
test.idx = folds[k,2][[1]][[toString(k)]]$idx
# Split up the data into the training and test datasets
train.X = data[train.idx,]
test.X = data[test.idx,]
train.y = resp[train.idx]
test.y = resp[test.idx]
# Perform the logistic-LASSO
fit = LogLASSO.CD(X = train.X, y = train.y, beta = start, lambda = l)
u = exp(as.matrix(test.X) %*% fit$coefficients)
z = u / (1 + u)
# Calculate the test MSE for the fold
fold.mse = mean((test.y - z)^2)
fold.mses = c(fold.mses, fold.mse)
}
fold.se = sqrt(var(fold.mses)/5)
path = rbind(path,
c(lambda = l, log.lambda = log(l),
avg.fold.mse = mean(fold.mses), avg.fold.se = fold.se))
print(paste("Iteration:", i, "done"))
}
return(as.tibble(path))
}
cv.path = lambda.cv(lambda.seq, coeffs, X, y)
min.mse = min(cv.path$avg.fold.mse)
min.lambda = cv.path[which(cv.path$avg.fold.mse == min.mse),]$log.lambda
cv.path %>%
ggplot(data = ., aes(x = log.lambda, y = avg.fold.mse)) +
geom_vline(xintercept = min.lambda) +
geom_line(color = "red") +
geom_errorbar(aes(ymin = avg.fold.mse - avg.fold.se, ymax = avg.fold.mse + avg.fold.se), color = "gray50") +
labs(
title = "Average Test Fold MSE as a function of lambda",
x = "log(lambda)",
y = "Average Test MSE"
) +
theme(plot.title = element_text(hjust = 0.5))
# Fit all three models
NR.fit = modified(X, y, logisticstuff, coeffs)
# Fit all three models
NR.fit = modified(as.matrix(X), y, logisticstuff, coeffs)
logisticstuff <- function(x, y, betavec) {
u <- x %*% betavec
expu <- exp(u)
loglik = vector(mode = "numeric", nrow(x))
for(i in 1:nrow(x))
loglik[i] = y[i]*u[i] - log(1 + expu[i])
loglik_value = sum(loglik)
# Log-likelihood at betavec
p <- expu / (1 + expu)
# P(Y_i=1|x_i)
grad = vector(mode = "numeric", length(betavec))
#grad[1] = sum(y - p)
for(i in 1:length(betavec))
grad[i] = sum(t(x[,i])%*%(y - p))
#Hess <- -t(x)%*%p%*%t(1-p)%*%x
Hess = hess_cal(x, p)
return(list(loglik = loglik_value, grad = grad, Hess = Hess))
}
hess_cal = function(x, p) {
len = length(p)
hess = matrix(0, ncol(x), ncol(x))
for (i in 1:len) {
unit = x[i,] %*% t(x[i,]) * p[i] *(1 - p[i])
#unit = t(x[i,])%*%x[i,]*p[i]*(1-p[i])
hess = hess + unit
}
return(-hess)
}
NewtonRaphson <- function(x, y, logisticstuff, start, tol = 1e-5, maxiter = 200) {
i <- 0
cur <- start
stuff <- logisticstuff(x, y, cur)
res = c(0, cur)
#res <- c(0, stuff$loglik, cur)
prevloglik <- -Inf      # To make sure it iterates
#while(i < maxiter && abs(stuff$loglik - prevloglik) > tol && stuff$loglik > -Inf)
while (i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
i <- i + 1
prevloglik <- stuff$loglik
print(prevloglik)
prev <- cur
cur <- prev - solve(stuff$Hess) %*% stuff$grad
stuff <- logisticstuff(x, y, cur)        # log-lik, gradient, Hessian
res = rbind(res, c(i, cur))
#res <- rbind(res, c(i, stuff$loglik, cur))
# Add current values to results matrix
}
return(res)
}
modified <- function(x, y, logisticstuff, start, tol = 1e-5, maxiter = 200){
i <- 0
cur <- start
beta_len <- length(start)
stuff <- logisticstuff(x, y, cur)
res = c(0, cur)
#res <- c(0, stuff$loglik,cur)
prevloglik <- -Inf # To make sure it iterates
while(i <= maxiter && abs(stuff$loglik - prevloglik) > tol)
#while(i <= maxiter && abs(stuff$loglik - prevloglik) > tol && stuff$loglik > -Inf)
{ i <- i + 1
prevloglik <- stuff$loglik
prev <- cur
lambda = 0
while (is.negative.definite(stuff$Hess-lambda*diag(beta_len)) == FALSE) {
lambda = lambda + 1
}
cur <- prev - solve(stuff$Hess-lambda*diag(beta_len)) %*% stuff$grad
#cur <- prev + (diag(beta_len)/10)%*%(stuff$grad)
#cur = prev + t(stuff$grad)%*%(stuff$grad)
stuff <- logisticstuff(x, y, cur) # log-lik, gradient, Hessian
res = rbind(res, c(i, cur))
#res <- rbind(res, c(i, stuff$loglik, cur))
}
return(res)
}
# Fit all three models
NR.fit = modified(as.matrix(X), y, logisticstuff, coeffs)
LL.fit = LogLASSO.CD(X, y, coeffs, exp(min.lambda))
View(X)
coeff.table = tibble(
`Coefficient` = c("Intercept", "Mean radius", "Mean texture", "Mean smoothness",
"Mean concavity", "Mean symmetry", "Mean fractal dimension",
"Standard error radius", "Standard error texture",
"Standard error smoothness"),
`Newton-Raphson` = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)],
`Logistic-LASSO` = LL.fit$coefficients
)
coeff.table = tibble(
`Coefficient` = c("Intercept", "Mean radius", "Mean texture", "Mean smoothness",
"Mean concavity", "Mean symmetry", "Mean fractal dimension",
"Standard error radius", "Standard error texture",
"Standard error smoothness", "Standard error concavity",
"Standard error symmetry"),
`Newton-Raphson` = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)],
`Logistic-LASSO` = LL.fit$coefficients
)
knitr::kable(coeff.table)
coeffs = rep(0.001, 13)
NR.coefs = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)]
# Set up the datasets for cross-validation
folds = crossv_kfold(cbp.X, k = 10)
coeffs = rep(0.001, 13)
NR.coefs = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)]
# Set up the datasets for cross-validation
folds = crossv_kfold(cbp.X, k = 10)
coeffs = rep(0.001, 13)
NR.coefs = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)]
# Set up the datasets for cross-validation
folds = crossv_kfold(X, k = 10)
NR.mses = NULL
LL.mses = NULL
for (k in 1:nrow(folds)) {
# Grab the specific training indexes
train.idx = folds[k,1][[1]][[toString(k)]]$idx
# Split up the data into the training and test datasets
train.X = X[train.idx,]
test.X = X[-train.idx,]
train.y = y[train.idx]
test.y = y[-train.idx]
LogLASSO = LogLASSO.CD(X = train.X, y = train.y,
beta = coeffs, lambda = exp(min.lambda))
LL.u = exp(test.X %*% LogLASSO$coefficients)
LL.z = LL.u / (1 + LL.u)
NR.u = exp(test.X %*% NR.coefs)
NR.z = NR.u / (1 + NR.u)
LL.mse = mean((test.y - LL.z)^2)
LL.mses = cbind(LL.mses, LL.mse)
NR.mse = mean((test.y - NR.z)^2)
NR.mses = cbind(NR.mses, NR.mse)
}
coeffs = rep(0.001, 12)
NR.coefs = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)]
# Set up the datasets for cross-validation
folds = crossv_kfold(X, k = 10)
NR.mses = NULL
LL.mses = NULL
for (k in 1:nrow(folds)) {
# Grab the specific training indexes
train.idx = folds[k,1][[1]][[toString(k)]]$idx
# Split up the data into the training and test datasets
train.X = X[train.idx,]
test.X = X[-train.idx,]
train.y = y[train.idx]
test.y = y[-train.idx]
LogLASSO = LogLASSO.CD(X = train.X, y = train.y,
beta = coeffs, lambda = exp(min.lambda))
LL.u = exp(test.X %*% LogLASSO$coefficients)
LL.z = LL.u / (1 + LL.u)
NR.u = exp(test.X %*% NR.coefs)
NR.z = NR.u / (1 + NR.u)
LL.mse = mean((test.y - LL.z)^2)
LL.mses = cbind(LL.mses, LL.mse)
NR.mse = mean((test.y - NR.z)^2)
NR.mses = cbind(NR.mses, NR.mse)
}
coeffs = rep(0.001, 12)
NR.coefs = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)]
# Set up the datasets for cross-validation
folds = crossv_kfold(X, k = 10)
NR.mses = NULL
LL.mses = NULL
for (k in 1:nrow(folds)) {
# Grab the specific training indexes
train.idx = folds[k,1][[1]][[toString(k)]]$idx
# Split up the data into the training and test datasets
train.X = X[train.idx,]
test.X = X[-train.idx,]
train.y = y[train.idx]
test.y = y[-train.idx]
LogLASSO = LogLASSO.CD(X = train.X, y = train.y,
beta = coeffs, lambda = exp(min.lambda))
LL.u = exp(as.matrix(test.X) %*% LogLASSO$coefficients)
LL.z = LL.u / (1 + LL.u)
NR.u = exp(as.matrix(test.X) %*% NR.coefs)
NR.z = NR.u / (1 + NR.u)
LL.mse = mean((test.y - LL.z)^2)
LL.mses = cbind(LL.mses, LL.mse)
NR.mse = mean((test.y - NR.z)^2)
NR.mses = cbind(NR.mses, NR.mse)
}
tidy.mses = tibble(
`Logistic LASSO` = c(LL.mses),
`Newton-Raphson` = c(NR.mses)
) %>%
gather(., key = "model", value = "test.MSE", `Logistic LASSO`:`Newton-Raphson`)
tidy.mses %>%
ggplot(data = ., aes(x = test.MSE, color = model, fill = model)) +
geom_histogram(bins = 200) +
theme(legend.position = "bottom") +
labs(
title = "Distribution of test MSE in 10-fold cross validation by model",
x = "Test MSE",
y = "Density"
) +
theme(plot.title = element_text(hjust = 0.5))
tidy.mses = tibble(
`Logistic LASSO` = c(LL.mses),
`Newton-Raphson` = c(NR.mses)
) %>%
gather(., key = "model", value = "test.MSE", `Logistic LASSO`:`Newton-Raphson`)
tidy.mses %>%
ggplot(data = ., aes(x = test.MSE, color = model, fill = model)) +
geom_density() +
theme(legend.position = "bottom") +
labs(
title = "Distribution of test MSE in 10-fold cross validation by model",
x = "Test MSE",
y = "Density"
) +
theme(plot.title = element_text(hjust = 0.5))
tidy.mses = tibble(
`Logistic LASSO` = c(LL.mses),
`Newton-Raphson` = c(NR.mses)
) %>%
gather(., key = "model", value = "test.MSE", `Logistic LASSO`:`Newton-Raphson`)
tidy.mses %>%
ggplot(data = ., aes(x = test.MSE, color = model, fill = model)) +
geom_density(alpha = 0.5) +
theme(legend.position = "bottom") +
labs(
title = "Distribution of test MSE in 10-fold cross validation by model",
x = "Test MSE",
y = "Density"
) +
theme(plot.title = element_text(hjust = 0.5))
glm.fit = cv.glmnet(as.matrix(X), as.matrix(y), alpha = 1, family = "binomial")
plot(glm.fit)
log.fit = glm(y ~ ., family = binomial(link = "logit"), data = X)
preds = predict(log.fit, X)
# Function for generating the path of solutions with different lambdas
create.sol.path = function(lambdas, start, data, resp) {
### Parameters:
# lambdas : the sequence of lambdas that you want to create solutions for
# start : the starting beta coefficients
# data : the data to estimate the coefficients from
# resp : the response variable you're trying to predict
# returns an matrix of the lambda and the computed beta coefficients for that lambda
coeff.path = NULL
coeffs = start
for (l in 1:length(lambdas)) {
fit = LogLASSO.CD(data, resp, coeffs, lambdas[l])
iter = fit$iter
obj = fit$obj
coeff.path = rbind(coeff.path, c(lambda = lambdas[l], iter = iter, obj = obj, fit$coefficients))
print(paste("Iter", l, "done,", iter, "loops needed for convergence",  sep = " ")) # progress bar
}
return(coeff.path)
}
# Create the solution path for the data
lambda.seq = exp(seq(-3, 6, length = 300))
coeffs = rep(0.001, 12)
path = create.sol.path(lambda.seq, coeffs, X, y)
tidy.path = as.tibble(path) %>%
gather(., key = "coeff", value = "coeff_est", intercept:symmetry_se) %>%
mutate(log.lambda = log(lambda))
ggplot(data = tidy.path, aes(x = log.lambda, y = coeff_est, color = coeff, group = coeff)) +
geom_line(alpha = 0.5) +
labs(
title = "Log-LASSO Coefficient estimates as a function of log(lambda)",
x = "log(lambda)",
y = "Coefficient estimate"
) +
theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
View(LogLASSO)
logisticstuff <- function(x, y, betavec) {
u <- x %*% betavec
expu <- exp(u)
loglik = vector(mode = "numeric", nrow(x))
for(i in 1:nrow(x))
loglik[i] = y[i]*u[i] - log(1 + expu[i])
loglik_value = sum(loglik)
# Log-likelihood at betavec
p <- expu / (1 + expu)
# P(Y_i=1|x_i)
grad = vector(mode = "numeric", length(betavec))
#grad[1] = sum(y - p)
for(i in 1:length(betavec))
grad[i] = sum(t(x[,i])%*%(y - p))
#Hess <- -t(x)%*%p%*%t(1-p)%*%x
Hess = hess_cal(x, p)
return(list(loglik = loglik_value, grad = grad, Hess = Hess))
}
hess_cal = function(x, p) {
len = length(p)
hess = matrix(0, ncol(x), ncol(x))
for (i in 1:len) {
unit = x[i,] %*% t(x[i,]) * p[i] *(1 - p[i])
#unit = t(x[i,])%*%x[i,]*p[i]*(1-p[i])
hess = hess + unit
}
return(-hess)
}
NewtonRaphson <- function(x, y, logisticstuff, start, tol = 1e-5, maxiter = 200) {
i <- 0
cur <- start
stuff <- logisticstuff(x, y, cur)
res = c(0, cur)
#res <- c(0, stuff$loglik, cur)
prevloglik <- -Inf      # To make sure it iterates
#while(i < maxiter && abs(stuff$loglik - prevloglik) > tol && stuff$loglik > -Inf)
while (i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
i <- i + 1
prevloglik <- stuff$loglik
print(prevloglik)
prev <- cur
cur <- prev - solve(stuff$Hess) %*% stuff$grad
stuff <- logisticstuff(x, y, cur)        # log-lik, gradient, Hessian
res = rbind(res, c(i, cur))
#res <- rbind(res, c(i, stuff$loglik, cur))
# Add current values to results matrix
}
return(res)
}
modified <- function(x, y, logisticstuff, start, tol = 1e-5, maxiter = 200){
i <- 0
cur <- start
beta_len <- length(start)
stuff <- logisticstuff(x, y, cur)
res = c(0, cur)
#res <- c(0, stuff$loglik,cur)
prevloglik <- -Inf # To make sure it iterates
while(i <= maxiter && abs(stuff$loglik - prevloglik) > tol)
#while(i <= maxiter && abs(stuff$loglik - prevloglik) > tol && stuff$loglik > -Inf)
{ i <- i + 1
prevloglik <- stuff$loglik
prev <- cur
lambda = 0
while (is.negative.definite(stuff$Hess-lambda*diag(beta_len)) == FALSE) {
lambda = lambda + 1
}
print(i)
cur <- prev - solve(stuff$Hess-lambda*diag(beta_len)) %*% stuff$grad
#cur <- prev + (diag(beta_len)/10)%*%(stuff$grad)
#cur = prev + t(stuff$grad)%*%(stuff$grad)
stuff <- logisticstuff(x, y, cur) # log-lik, gradient, Hessian
res = rbind(res, c(i, cur))
#res <- rbind(res, c(i, stuff$loglik, cur))
}
return(res)
}
# Fit all three models
NR.fit = modified(as.matrix(X), y, logisticstuff, coeffs)
LL.fit = LogLASSO.CD(X, y, coeffs, exp(min.lambda))
exp(min.lambda)
