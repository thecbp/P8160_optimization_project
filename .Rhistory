stdev = sd(col)
return((col - mean)/stdev)
}
# just standardize the covariates
standardized.data = read.csv(file = "breast-cancer-1.csv") %>%
dplyr::select(radius_mean:fractal_dimension_worst) %>%
map_df(.x = ., standardize)
# add back in the response and ids
data = cbind(read.csv(file = "breast-cancer-1.csv") %>% dplyr::select(diagnosis), standardized.data) %>%
mutate(diagnosis = ifelse(diagnosis == "M", 1, 0))
soft.threshold = function(beta, gamma) {
new.beta = beta
if (abs(beta) > gamma && beta > 0) {
new.beta = beta - gamma
} else if (abs(beta) > gamma && beta < 0) {
new.beta = beta + gamma
} else {
new.beta = 0
}
return(new.beta)
}
calc.cur.p = function(intercept, data, betas) {
# return n x 1 array of current probabilities evaluated with given betas
return(
exp(intercept * rep(1, nrow(data)) + data %*% betas) / (1 + exp(intercept * rep(1, nrow(data)) + data %*% betas))
)
}
calc.working.resp = function(intercept, data, resp, betas, p) {
# return n x 1 array of working responses evaluated with given betas
return(
intercept * rep(1, nrow(data)) + data %*% betas + (resp - p) / (p * (1 - p))
)
}
calc.working.weights = function(p) {
# return n x 1 array of working weights for the data
return(p * (1 - p))
}
calc.obj = function(data, weights, w.resp, intercept, betas, lambda) {
# return the objective function of data and current params
return(
log.lik = 1/(2 * nrow(data)) *
sum((weights * (w.resp - intercept * rep(1, nrow(data)) - data %*% betas))^2) + lambda * sum(abs(betas))
)
}
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
beta = as.matrix(beta)
# Initialize important parameters before starting the coordinate descent
beta0 = 1/length(y) * sum(y - X %*% beta)
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, beta, obj = obj)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
r = y - (X %*% beta) + (X[,k] * beta[k])
threshold.val = sum(omega * X[,k] * r)
beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)
}
# With new betas, recalculate the working parameters
beta0 = mean(y) - sum(colMeans(X) * beta)
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Append it to tracking matrix
path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
# Break the loop if the diff between likelihoods is below tolerance
if (
norm(prev.beta - beta, "F") < tol
) { break }
}
return(list(
path = as.tibble(path),
coefficients = c(beta0, beta))
)
}
X = data %>% dplyr::select(radius_mean:fractal_dimension_worst)
y = data$diagnosis
# initial guess will be what is produced by regular linear regression
ls.beta = lm(diagnosis ~ ., data = data)
ls.coeffs = as.matrix(ls.beta$coefficients[2:length(ls.beta$coefficients)])
lambda = 0.12
cancer.CD = LogLASSO.CD(X = X, y = y, beta = ls.coeffs, lambda = lambda, tol = 1e-4)
a = c()
set.seed(8160)
# lambdas to cross-validate against
beta = lm(diagnosis ~ ., data = data)
ls.coeffs = beta$coefficients[2:length(ls.beta$coefficients)]
lambda0 = max(ls.coeffs) # previously calculated as max of the LS betas
lambda.seq = exp(seq(-5, lambda0, length = 2))
avg.rmses = NULL
for (l in 1:lambda.seq) {
folds = data %>% crossv_kfold(., k = 5)
rmses = NULL
for (k in 1:nrow(folds)) {
train = folds[k,1][[1]][[toString(k)]]$data
X = train %>% dplyr::select(radius_mean:fractal_dimension_worst)
y = train$diagnosis
test = folds[k,2][[1]][[toString(k)]]$data
test.X = test %>% dplyr::select(radius_mean:fractal_dimension_worst)
test.y = test$diagnosis
LogLASSO = LogLASSO.CD(X = X, y = y, beta = ls.coeffs, lambda = lambda.seq[l])
LL.coefs = LogLASSO$coefficients
rmse = sum(sqrt((test.y - as.matrix(cbind(1 * rep(1, nrow(test.X)), test.X)) %*% LL.coefs)^2))
rmses = cbind(rmses, rmse)
}
avg.rmses = cbind(avg.rmses, mean(rmses))
print(paste("iter", l, "done"))
}
View(avg.rmses)
library(tidyverse)
library(modelr)
standardize = function(col) {
mean = mean(col)
stdev = sd(col)
return((col - mean)/stdev)
}
# just standardize the covariates
standardized.data = read.csv(file = "breast-cancer-1.csv") %>%
dplyr::select(radius_mean:fractal_dimension_worst) %>%
map_df(.x = ., standardize)
# add back in the response and ids
data = cbind(read.csv(file = "breast-cancer-1.csv") %>% dplyr::select(diagnosis), standardized.data) %>%
mutate(diagnosis = ifelse(diagnosis == "M", 1, 0))
folds = crossv_kfold(data, k = 5)
View(folds)
folds = crossv_kfold(data, k = 5)
a = folds[1,1][[1]][["1"]]$data
folds = crossv_kfold(data, k = 5)
a = folds[1,1][[1]][["1"]]$data
b = folds[2,1][[1]][["1"]]$data
folds = crossv_kfold(data, k = 5)
a = folds[1,1][[1]][["1"]]$data
b = folds[2,1][[1]][["2"]]$data
library(caret)
caret.folds = createFolds(data, k = 5)
View(caret.folds)
library(caret)
caret.folds = createFolds(data, k = 5, list = TRUE)
View(caret.folds)
library(caret)
caret.folds = createFolds(data, k = 5, list = TRUE, returnTrain = FALSE)
View(caret.folds)
caret.folds[["Fold4"]]
caret.folds[["Fold5"]]
caret.folds[["Fold4"]]
library(caret)
caret.folds = createFolds(data, k = 5, list = TRUE, returnTrain = TRUE)
View(caret.folds)
names(flds)[1] <- "train"
View(caret.folds)
caret.folds[["Fold1"]]
caret.folds[["Fold2"]]
caret.folds[["Fold3"]]
part = createDataPartition(data, times = 5)
View(caret.folds)
library(tidyverse)
library(modelr)
standardize = function(col) {
mean = mean(col)
stdev = sd(col)
return((col - mean)/stdev)
}
# just standardize the covariates
standardized.data = read.csv(file = "breast-cancer-1.csv") %>%
dplyr::select(radius_mean:fractal_dimension_worst) %>%
map_df(.x = ., standardize)
# add back in the response and ids
data = cbind(read.csv(file = "breast-cancer-1.csv") %>% dplyr::select(diagnosis), standardized.data) %>%
mutate(diagnosis = ifelse(diagnosis == "M", 1, 0))
library(tidyverse)
library(modelr)
library(caret)
cfolds = createFolds(data, k = 5, returnTrain = TRUE)
View(cfolds)
cfolds["Fold1"]
cfolds["Fold2"]
cfolds["Fold3"]
cfolds["Fold4"]
cfolds["Fold5"]
cfolds = createFolds(data, k = 10, returnTrain = TRUE)
View(cfolds)
View(caret.folds)
cfolds = createFolds(data, k = 10, returnTrain = TRUE)
View(cfolds)
?createFolds
soft.threshold = function(beta, gamma) {
new.beta = beta
if (abs(beta) > gamma && beta > 0) {
new.beta = beta - gamma
} else if (abs(beta) > gamma && beta < 0) {
new.beta = beta + gamma
} else {
new.beta = 0
}
return(new.beta)
}
calc.cur.p = function(intercept, data, betas) {
# return n x 1 array of current probabilities evaluated with given betas
return(
exp(intercept * rep(1, nrow(data)) + data %*% betas) / (1 + exp(intercept * rep(1, nrow(data)) + data %*% betas))
)
}
calc.working.resp = function(intercept, data, resp, betas, p) {
# return n x 1 array of working responses evaluated with given betas
return(
intercept * rep(1, nrow(data)) + data %*% betas + (resp - p) / (p * (1 - p))
)
}
calc.working.weights = function(p) {
# return n x 1 array of working weights for the data
return(p * (1 - p))
}
calc.obj = function(data, weights, w.resp, intercept, betas, lambda) {
# return the objective function of data and current params
return(
log.lik = 1/(2 * nrow(data)) *
sum((weights * (w.resp - intercept * rep(1, nrow(data)) - data %*% betas))^2) + lambda * sum(abs(betas))
)
}
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
beta = as.matrix(beta)
# Initialize important parameters before starting the coordinate descent
beta0 = 1/length(y) * sum(y - X %*% beta)
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, beta, obj = obj)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
r = y - (X %*% beta) + (X[,k] * beta[k])
threshold.val = sum(omega * X[,k] * r)
beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)
}
# With new betas, recalculate the working parameters
beta0 = mean(y) - sum(colMeans(X) * beta)
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Append it to tracking matrix
path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
# Break the loop if the diff between likelihoods is below tolerance
if (
norm(prev.beta - beta, "F") < tol
) { break }
}
return(list(
path = as.tibble(path),
coefficients = c(beta0, beta))
)
}
X = data %>% dplyr::select(radius_mean:fractal_dimension_worst)
y = data$diagnosis
# initial guess will be what is produced by regular linear regression
ls.beta = lm(diagnosis ~ ., data = data)
ls.coeffs = as.matrix(ls.beta$coefficients[2:length(ls.beta$coefficients)])
lambda = 0.12
#cancer.CD = LogLASSO.CD(X = X, y = y, beta = ls.coeffs, lambda = lambda, tol = 1e-4)
a = createFolds(y)
a = createFolds(y, k = 5)
View(a)
crossv_kfold()
?crossv_kfold
a = createFolds(y, k = 5)
cv = crossv_kfold(data, k = 5)
View(cv)
dim(cv[1,1][[1]][["1"]]$data)
dim(cv[1,1][[1]][["1"]]
)
a = cv[1,1][[1]][["1"]]
View(a)
b = cv[1,1][[1]][["1"]]
View(b)
b[["idx"]]
cv
cv[1,1]
cv[1,1][[1]]
cv[1,1][[1]][[`1`]]
cv[1,1][[1]]
length(cv[1,1][[1]])
d = cv[1,1][[1]]
View(d)
View(d$idx)
View(d[["1"]]$idx)
set.seed(8160)
# lambdas to cross-validate against
beta = lm(diagnosis ~ ., data = data)
ls.coeffs = beta$coefficients[2:length(ls.beta$coefficients)]
lambda0 = max(ls.coeffs) # previously calculated as max of the LS betas
lambda.seq = exp(seq(-5, lambda0, length = 3))
avg.rmses = NULL
for (l in 1:lambda.seq) {
folds = crossv_kfold(data, k = 5)
rmses = NULL
for (k in 1:nrow(folds)) {
# Get the indexes given by the fold of the iteration
train.idx = folds[k,1][[1]][[toString(k)]]$idx
train = data[train.idx,]
X = train %>% dplyr::select(radius_mean:fractal_dimension_worst)
y = train$diagnosis
test = data[-train.idx,]
test.X = test %>% dplyr::select(radius_mean:fractal_dimension_worst)
test.y = test$diagnosis
LogLASSO = LogLASSO.CD(X = X, y = y, beta = ls.coeffs, lambda = lambda.seq[l])
LL.coefs = LogLASSO$coefficients
rmse = sum(sqrt((test.y - as.matrix(cbind(1 * rep(1, nrow(test.X)), test.X)) %*% LL.coefs)^2))
rmses = cbind(rmses, rmse)
}
avg.rmses = cbind(avg.rmses, mean(rmses))
print(paste("iter", l, "done"))
}
View(avg.rmses)
set.seed(8160)
# lambdas to cross-validate against
beta = lm(diagnosis ~ ., data = data)
ls.coeffs = beta$coefficients[2:length(ls.beta$coefficients)]
lambda0 = max(ls.coeffs) # previously calculated as max of the LS betas
lambda.seq = exp(seq(-5, lambda0, length = 3))
avg.rmses = NULL
for (l in lambda.seq) {
folds = crossv_kfold(data, k = 5)
rmses = NULL
for (k in 1:nrow(folds)) {
# Get the indexes given by the fold of the iteration
train.idx = folds[k,1][[1]][[toString(k)]]$idx
train = data[train.idx,]
X = train %>% dplyr::select(radius_mean:fractal_dimension_worst)
y = train$diagnosis
test = data[-train.idx,]
test.X = test %>% dplyr::select(radius_mean:fractal_dimension_worst)
test.y = test$diagnosis
LogLASSO = LogLASSO.CD(X = X, y = y, beta = ls.coeffs, lambda = l)
LL.coefs = LogLASSO$coefficients
rmse = sum(sqrt((test.y - as.matrix(cbind(1 * rep(1, nrow(test.X)), test.X)) %*% LL.coefs)^2))
rmses = cbind(rmses, rmse)
}
avg.rmses = cbind(avg.rmses, mean(rmses))
print(paste("iter", l, "done"))
}
plot.lambda = tibble(
lambdas = lambda.seq,
avg.test.MSE = c(avg.mses)
)
plot.lambda = tibble(
lambdas = lambda.seq,
avg.test.MSE = c(avg.rmses)
)
ggplot(data = plot.lambda, aes(x = log(lambdas), y = avg.test.MSE)) +
geom_point() +
labs(
title = "Average test MSE as a function of log(lambda)",
x = "log(lambda)",
y = "Average Test MSE"
)
plot.lambda = tibble(
lambdas = lambda.seq,
avg.test.MSE = c(avg.rmses)
)
ggplot(data = plot.lambda, aes(x = log(lambdas), y = avg.test.MSE)) +
geom_line() +
labs(
title = "Average test MSE as a function of log(lambda)",
x = "log(lambda)",
y = "Average Test MSE"
)
set.seed(8160)
# lambdas to cross-validate against
beta = lm(diagnosis ~ ., data = data)
ls.coeffs = beta$coefficients[2:length(ls.beta$coefficients)]
lambda0 = max(ls.coeffs) # previously calculated as max of the LS betas
lambda.seq = exp(seq(-5, lambda0, length = 100))
avg.rmses = NULL
for (l in lambda.seq) {
folds = crossv_kfold(data, k = 5)
rmses = NULL
for (k in 1:nrow(folds)) {
# Get the indexes given by the fold of the iteration
train.idx = folds[k,1][[1]][[toString(k)]]$idx
train = data[train.idx,]
X = train %>% dplyr::select(radius_mean:fractal_dimension_worst)
y = train$diagnosis
test = data[-train.idx,]
test.X = test %>% dplyr::select(radius_mean:fractal_dimension_worst)
test.y = test$diagnosis
LogLASSO = LogLASSO.CD(X = X, y = y, beta = ls.coeffs, lambda = l)
LL.coefs = LogLASSO$coefficients
rmse = sum(sqrt((test.y - as.matrix(cbind(1 * rep(1, nrow(test.X)), test.X)) %*% LL.coefs)^2))
rmses = cbind(rmses, rmse)
}
avg.rmses = cbind(avg.rmses, mean(rmses))
print(paste("iter", l, "done"))
}
plot.lambda = tibble(
lambdas = lambda.seq,
avg.test.MSE = c(avg.rmses)
)
ggplot(data = plot.lambda, aes(x = log(lambdas), y = avg.test.MSE)) +
geom_line() +
labs(
title = "Average test MSE as a function of log(lambda)",
x = "log(lambda)",
y = "Average Test MSE"
)
set.seed(8160)
# lambdas to cross-validate against
beta = lm(diagnosis ~ ., data = data)
ls.coeffs = beta$coefficients[2:length(ls.beta$coefficients)]
lambda0 = max(ls.coeffs) # previously calculated as max of the LS betas
lambda.seq = exp(seq(-5, lambda0, length = 100))
avg.rmses = NULL
# Set up the datasets for cross-validation
folds = crossv_kfold(data, k = 5)
train.idx = folds[k,1][[1]][[toString(k)]]$idx
train = data[train.idx,]
train.X = train %>% dplyr::select(radius_mean:fractal_dimension_worst)
train.y = train$diagnosis
test = data[-train.idx,]
test.X = test %>% dplyr::select(radius_mean:fractal_dimension_worst)
test.y = test$diagnosis
for (l in lambda.seq) {
rmses = NULL
for (k in 1:nrow(folds)) {
LogLASSO = LogLASSO.CD(X = X, y = y, beta = ls.coeffs, lambda = l)
LL.coefs = LogLASSO$coefficients
rmse = sum(sqrt((test.y - as.matrix(cbind(1 * rep(1, nrow(test.X)), test.X)) %*% LL.coefs)^2))
rmses = cbind(rmses, rmse)
}
avg.rmses = cbind(avg.rmses, mean(rmses))
print(paste("iter", l, "done"))
}
set.seed(8160)
# lambdas to cross-validate against
beta = lm(diagnosis ~ ., data = data)
ls.coeffs = beta$coefficients[2:length(ls.beta$coefficients)]
lambda0 = max(ls.coeffs) # previously calculated as max of the LS betas
lambda.seq = exp(seq(-5, lambda0, length = 100))
avg.rmses = NULL
# Set up the datasets for cross-validation
folds = crossv_kfold(data, k = 5)
train.idx = folds[k,1][[1]][[toString(k)]]$idx
train = data[train.idx,]
train.X = train %>% dplyr::select(radius_mean:fractal_dimension_worst)
train.y = train$diagnosis
test = data[-train.idx,]
test.X = test %>% dplyr::select(radius_mean:fractal_dimension_worst)
test.y = test$diagnosis
for (l in lambda.seq) {
rmses = NULL
for (k in 1:nrow(folds)) {
LogLASSO = LogLASSO.CD(X = train.X, y = train.y, beta = ls.coeffs, lambda = l)
LL.coefs = LogLASSO$coefficients
rmse = sum(sqrt((test.y - as.matrix(cbind(1 * rep(1, nrow(test.X)), test.X)) %*% LL.coefs)^2))
rmses = cbind(rmses, rmse)
}
avg.rmses = cbind(avg.rmses, mean(rmses))
print(paste("iter", l, "done"))
}
plot.lambda = tibble(
lambdas = lambda.seq,
avg.test.MSE = c(avg.rmses)
)
ggplot(data = plot.lambda, aes(x = log(lambdas), y = avg.test.MSE)) +
geom_line() +
labs(
title = "Average test MSE as a function of log(lambda)",
x = "log(lambda)",
y = "Average Test MSE"
)
min.RMSE = min(plot.lambda$avg.test.MSE)
min.RMSE = min(plot.lambda$avg.test.MSE)
plot.lambda[which(plot.lambda$avg.test.MSE == min.RMSE)]
min.RMSE = min(plot.lambda$avg.test.MSE)
plot.lambda[which(plot.lambda$avg.test.MSE == min.RMSE),]
min.RMSE = min(plot.lambda$avg.test.MSE)
min.lambda = plot.lambda[which(plot.lambda$avg.test.MSE == min.RMSE),]$lambdas
