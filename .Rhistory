log.lasso.coefs = coef(log.lasso)
plot(log.lasso, xvar="lambda")
View(log.lasso)
View(log.lasso.coefs)
# Sanity checks for seeing if the algorithms are correct
X = cbp.X %>% as.matrix(.)
Y = as.matrix(cbp.y)
log.lasso = glmnet(X, Y, alpha = 1, family = "binomial")
log.lasso.coefs = matrix(coef(log.lasso))
plot(log.lasso, xvar="lambda")
View(log.lasso.coefs)
# Sanity checks for seeing if the algorithms are correct
X = cbp.X %>% as.matrix(.)
Y = as.matrix(cbp.y)
log.lasso = cv.glmnet(X, Y, alpha = 1, family = "binomial")
log.lasso.coefs = matrix(coef(log.lasso))
plot(log.lasso, xvar="lambda")
View(log.lasso.coefs)
# Sanity checks for seeing if the algorithms are correct
X = cbp.X %>% as.matrix(.)
Y = as.matrix(cbp.y)
log.lasso = cv.glmnet(X, Y, alpha = 1, family = "binomial")
log.lasso.coefs = matrix(coef(log.lasso))
plot(log.lasso)
View(log.lasso.coefs)
View(log.lasso)
# Generate data to visualize how the coefficients change with the logistic LASSO
lambda.seq = exp(seq(-10, 2, length = 500))
start.betas = rep(0.001,13)
coeff.path = NULL
for (l in 1:length(lambda.seq)) {
fit = LogLASSO.CD(X = cbp.X, y = cbp.y, beta = start.betas, lambda = lambda.seq[l])
coeff.path = rbind(coeff.path, c(lambda = lambda.seq[l], fit$coefficients))
print(paste("Iter", l, "done", sep = " ")) # progress bar
}
#colnames(coeff.path) = c("lambda", paste("V", 1:13, sep = ""))
tidy.lambda = as.tibble(coeff.path) %>%
gather(., key = "coeff", value = "coeff_est", V1:V13) %>%
mutate(
log.lambda = log(lambda)
)
ggplot(data = tidy.lambda, aes(x = log.lambda, y = coeff_est, color = coeff, group = coeff)) +
geom_line(alpha = 0.5) +
theme(legend.position = "right") +
labs(
title = "Log-LASSO Coefficient estimates as a function of log(lambda)",
x = "log(lambda)",
y = "Coefficient estimate"
)
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
# exclude the intercept from the input betas
beta = as.matrix(beta[2:length(beta)])
# Initialize important parameters before starting the coordinate descent
beta0 = 1/length(y) * sum(y - X %*% beta)
#beta0 = as.matrix(beta[1])
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, obj = obj, beta)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
diff.k = z  - (X[,-k] * beta[-k]) - beta0 * rep(1, length(y))
threshold.val = sum(omega * X[,k] * diff.k)
beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)
}
# With new betas, recalculate the working parameters
beta0 = mean(y) - sum(colMeans(X) * beta)
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Append it to tracking matrix
path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
# Break the loop if the diff between likelihoods is below tolerance
if (norm(prev.beta - beta, "F") < tol) { break }
}
return(list(
path = as.tibble(path),
coefficients = c(beta0, beta))
)
}
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
# exclude the intercept from the input betas
beta = as.matrix(beta[2:length(beta)])
# Initialize important parameters before starting the coordinate descent
beta0 = 1/length(y) * sum(y - X %*% beta)
#beta0 = as.matrix(beta[1])
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, obj = obj, beta)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
diff.k = z  - (X[,-k] * beta[-k]) - beta0 * rep(1, length(y))
threshold.val = sum(omega * X[,k] * diff.k)
beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)
}
# With new betas, recalculate the working parameters
beta0 = mean(y) - sum(colMeans(X) * beta)
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Append it to tracking matrix
path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
# Break the loop if the diff between likelihoods is below tolerance
if (norm(prev.beta - beta, "F") < tol) { break }
}
return(list(
path = as.tibble(path),
coefficients = c(beta0, beta))
)
}
# Generate data to visualize how the coefficients change with the logistic LASSO
lambda.seq = exp(seq(-10, 2, length = 500))
start.betas = rep(0.001,13)
coeff.path = NULL
for (l in 1:length(lambda.seq)) {
fit = LogLASSO.CD(X = cbp.X, y = cbp.y, beta = start.betas, lambda = lambda.seq[l])
coeff.path = rbind(coeff.path, c(lambda = lambda.seq[l], fit$coefficients))
print(paste("Iter", l, "done", sep = " ")) # progress bar
}
a = c(1,2,3,4)
a[-2]
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
# exclude the intercept from the input betas
beta = as.matrix(beta[2:length(beta)])
# Initialize important parameters before starting the coordinate descent
beta0 = 1/length(y) * sum(y - X %*% beta)
#beta0 = as.matrix(beta[1])
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, obj = obj, beta)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
diff.k = z - (X[,-k] %*% beta-[k]) - beta0 * rep(1, length(y))
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
# exclude the intercept from the input betas
beta = as.matrix(beta[2:length(beta)])
# Initialize important parameters before starting the coordinate descent
beta0 = 1/length(y) * sum(y - X %*% beta)
#beta0 = as.matrix(beta[1])
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, obj = obj, beta)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
diff.k = z - (X[,-k] %*% beta[-k]) - beta0 * rep(1, length(y))
threshold.val = sum(omega * X[,k] * diff.k)
beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)
}
# With new betas, recalculate the working parameters
beta0 = mean(y) - sum(colMeans(X) * beta)
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Append it to tracking matrix
path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
# Break the loop if the diff between likelihoods is below tolerance
if (norm(prev.beta - beta, "F") < tol) { break }
}
return(list(
path = as.tibble(path),
coefficients = c(beta0, beta))
)
}
# Generate data to visualize how the coefficients change with the logistic LASSO
lambda.seq = exp(seq(-10, 2, length = 500))
start.betas = rep(0.001,13)
coeff.path = NULL
for (l in 1:length(lambda.seq)) {
fit = LogLASSO.CD(X = cbp.X, y = cbp.y, beta = start.betas, lambda = lambda.seq[l])
coeff.path = rbind(coeff.path, c(lambda = lambda.seq[l], fit$coefficients))
print(paste("Iter", l, "done", sep = " ")) # progress bar
}
soft.threshold = function(beta, gamma) {
# Takes in a single beta and returns a reduced form of it
if (abs(beta) > gamma && beta > 0) {
new.beta = beta - gamma
} else if (abs(beta) > gamma && beta < 0) {
new.beta = beta + gamma
} else {
new.beta = 0
}
return(new.beta)
}
calc.cur.p = function(intercept, data, betas) {
# return n x 1 array of current probabilities evaluated with given betas
return(
exp(intercept * rep(1, nrow(data)) + data %*% betas) / (1 + exp(intercept * rep(1, nrow(data)) + data %*% betas))
)
}
calc.working.resp = function(intercept, data, resp, betas, p) {
# return n x 1 array of working responses evaluated with given betas
return(
intercept * rep(1, nrow(data)) + data %*% betas + (resp - p) / (p * (1 - p))
)
}
calc.working.weights = function(p) {
# return n x 1 array of working weights for the data
return(p * (1 - p))
}
calc.obj = function(data, weights, w.resp, intercept, betas, lambda) {
# return the objective function of data and current params
return(
log.lik = 1/(2 * nrow(data)) *
sum((weights * (w.resp - intercept * rep(1, nrow(data)) - data %*% betas))^2) + lambda * sum(abs(betas))
)
}
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
# exclude the intercept from the input betas
beta = as.matrix(beta[2:length(beta)])
# Initialize important parameters before starting the coordinate descent
beta0 = sum(y - X%*%beta)/(length(y))
#beta0 = as.matrix(beta[1])
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, obj = obj, beta)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
#diff.k = y - (X[,-k] %*% beta[-k]) + (X[,-k] * beta[-k]) # old version
diff.k = z - (X[,-k] %*% beta[-k]) + (X[,-k] * beta[-k])
threshold.val = sum(omega * X[,k] * diff.k)
beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)
}
# With new betas, recalculate the working parameters
beta0 = sum(y - X %*% beta)/(length(y))
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Append it to tracking matrix
path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
# Break the loop if the diff between likelihoods is below tolerance
if (norm(prev.beta - beta, "F") < tol) { break }
}
return(list(
path = as.tibble(path),
coefficients = c(beta0, beta))
)
}
# Generate data to visualize how the coefficients change with the logistic LASSO
lambda.seq = exp(seq(-10, 2, length = 500))
start.betas = rep(0.001,13)
coeff.path = NULL
for (l in 1:length(lambda.seq)) {
fit = LogLASSO.CD(X = cbp.X, y = cbp.y, beta = start.betas, lambda = lambda.seq[l])
coeff.path = rbind(coeff.path, c(lambda = lambda.seq[l], fit$coefficients))
print(paste("Iter", l, "done", sep = " ")) # progress bar
}
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
# exclude the intercept from the input betas
beta = as.matrix(beta[2:length(beta)])
# Initialize important parameters before starting the coordinate descent
beta0 = sum(y - X%*%beta)/(length(y))
#beta0 = as.matrix(beta[1])
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, obj = obj, beta)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
#diff.k = y - (X[,-k] %*% beta[-k]) + (X[,-k] * beta[-k]) # old version
diff.k = z - (X %*% beta) + (X[,-k] * beta[-k])
threshold.val = sum(omega * X[,k] * diff.k)
beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)
}
# With new betas, recalculate the working parameters
beta0 = sum(y - X %*% beta)/(length(y))
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Append it to tracking matrix
path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
# Break the loop if the diff between likelihoods is below tolerance
if (norm(prev.beta - beta, "F") < tol) { break }
}
return(list(
path = as.tibble(path),
coefficients = c(beta0, beta))
)
}
# Generate data to visualize how the coefficients change with the logistic LASSO
lambda.seq = exp(seq(-10, 2, length = 500))
start.betas = rep(0.001,13)
coeff.path = NULL
for (l in 1:length(lambda.seq)) {
fit = LogLASSO.CD(X = cbp.X, y = cbp.y, beta = start.betas, lambda = lambda.seq[l])
coeff.path = rbind(coeff.path, c(lambda = lambda.seq[l], fit$coefficients))
print(paste("Iter", l, "done", sep = " ")) # progress bar
}
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
### Parameters: #####################################################
# X : design matrix                                                 #
# y : response variable (should be binary)                          #
# beta : starting beta coefficients to start from                   #
# lambda : constraining parameter for LASSO penalization            #
# tol : how precise should our convergence be                       #
# maxiter : how many iterations should be performed before stopping #
#####################################################################
# Turn the betas into their own matrix
X = as.matrix(X)
# exclude the intercept from the input betas
beta = as.matrix(beta[2:length(beta)])
# Initialize important parameters before starting the coordinate descent
beta0 = sum(y - X%*%beta)/(length(y))
#beta0 = as.matrix(beta[1])
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Initialize the row for tracking each of these parameters
path = c(iter = 0, intercept = beta0, obj = obj, beta)
for (j in 1:maxiter) {
prev.beta = beta
# Coordinate descent
for (k in 1:length(beta)) {
#diff.k = y - (X[,-k] %*% beta[-k]) + (X[,k] * beta[k]) # old version
diff.k = z - (X %*% beta) + (X[,k] * beta[k])
threshold.val = sum(omega * X[,k] * diff.k)
beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)
}
# With new betas, recalculate the working parameters
beta0 = sum(y - X %*% beta)/(length(y))
p = calc.cur.p(intercept = beta0, data = X, betas = beta)
z = calc.working.resp(intercept = beta0, data = X, resp = y,
betas = beta, p = p)
omega = calc.working.weights(p)
obj = calc.obj(data = X, weights = omega, w.resp = z,
intercept = beta0, betas = beta, lambda = lambda)
# Append it to tracking matrix
path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
# Break the loop if the diff between likelihoods is below tolerance
if (norm(prev.beta - beta, "F") < tol) { break }
}
return(list(
path = as.tibble(path),
coefficients = c(beta0, beta))
)
}
# Generate data to visualize how the coefficients change with the logistic LASSO
lambda.seq = exp(seq(-10, 2, length = 500))
start.betas = rep(0.001,13)
coeff.path = NULL
for (l in 1:length(lambda.seq)) {
fit = LogLASSO.CD(X = cbp.X, y = cbp.y, beta = start.betas, lambda = lambda.seq[l])
coeff.path = rbind(coeff.path, c(lambda = lambda.seq[l], fit$coefficients))
print(paste("Iter", l, "done", sep = " ")) # progress bar
}
#colnames(coeff.path) = c("lambda", paste("V", 1:13, sep = ""))
tidy.lambda = as.tibble(coeff.path) %>%
gather(., key = "coeff", value = "coeff_est", V1:V13) %>%
mutate(
log.lambda = log(lambda)
)
ggplot(data = tidy.lambda, aes(x = log.lambda, y = coeff_est, color = coeff, group = coeff)) +
geom_line(alpha = 0.5) +
theme(legend.position = "right") +
labs(
title = "Log-LASSO Coefficient estimates as a function of log(lambda)",
x = "log(lambda)",
y = "Coefficient estimate"
)
avg.rmses = NULL
start.betas = rep(0.001, 13)
# Set up the datasets for cross-validation
folds = crossv_kfold(short.data, k = 5)
for (l in lambda.seq) {
rmses = NULL
for (k in 1:nrow(folds)) {
train.idx = folds[k,1][[1]][[toString(k)]]$idx
train = short.data[train.idx,]
test = short.data[-train.idx,]
train.X = train %>%
dplyr::select(which(colnames(short.data) %in% needed.cols)) %>%
dplyr::select(-diagnosis)
train.y = train$diagnosis
test.X = test %>%
dplyr::select(which(colnames(short.data) %in% needed.cols)) %>%
dplyr::select(-diagnosis)
test.y = test$diagnosis
LogLASSO = LogLASSO.CD(X = train.X, y = train.y,
beta = start.betas, lambda = l)
LL.coefs = LogLASSO$coefficients
rmse = sum(sqrt((test.y - as.matrix(cbind(1 * rep(1, nrow(test.X)), test.X)) %*% LL.coefs)^2))
rmses = cbind(rmses, rmse)
}
avg.rmses = cbind(avg.rmses, mean(rmses))
print(paste("iter: lambda = ", l, "done"))
}
View(LogLASSO)
View(LL.coefs)
# Sanity checks for seeing if the algorithms are correct
X = cbp.X %>% as.matrix(.)
Y = as.matrix(cbp.y)
glmnet.lambda.min = cv.glmnet(X, Y, alpha = 1, family = "binomial")$lambda.min
log.lasso = glmnet(X, Y, alpha = 1, family = "binomial", lambda = glmnet.lambda.min)
log.lasso.coefs = matrix(coef(log.lasso))
plot(log.lasso)
