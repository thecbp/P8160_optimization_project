---
title: 'Data Science II: HW 1'
author: "Christian Pascual"
date: "2/18/2019"
output: pdf_document
---

# Library setup and data import

```{r setup, message = FALSE }
library(tidyverse)
library(glmnet)
library(caret)
library(modelr)
library(pls)

set.seed(8160)
```

```{r data-import }
train = read.csv(file = "./solubility_train.csv") %>% janitor::clean_names()
test = read.csv(file = "./solubility_test.csv") %>% janitor::clean_names()
```

# Part a): Linear Squares Model

For the least squares model, I'll prepare a model using a two part process. First, I'll use stepwise regression to select a subset of variables from the set of 226, using AIC as the criterion. Then, I'll use 10-fold cross-validation to get better estimates for each of the coefficients. 

```{r part-1 }
ls.model = lm(solubility ~ ., data = train)
```

```{r part-1-mse }
MSE.calculation = test %>% 
  add_predictions(ls.model) %>% 
  mutate(MSE = (solubility - pred)^2)

ls.MSE = mean(MSE.calculation$MSE)
```

The test MSE for the least squares model is `r round(ls.MSE, 3)`.

# Part b): Ridge Regression

For the Ridge Regression, I'll test $\lambda$ values from $e^{-10}$ to $e^{10}$ to find an optimal tuning parameter.

```{r part-2 }
# set up data into correct format for glmnet
predictors = train %>% select(fp001:surface_area2) %>% as.matrix(.)
response = train$solubility
lambdas = exp(seq(-10, 10, length = 10000))

# perform the ridge regression CV for lambda
ridge.model = cv.glmnet(predictors, response, 
                        alpha = 0, lambda = lambdas, type.measure = "mse")
```

```{r part-2-mse }
best.ridge.lambda = ridge.model$lambda.1se
best.ridge.coeffs = predict(ridge.model, s = best.ridge.lambda, type = "coefficients")

test.predictors = test %>% 
  select(fp001:surface_area2)

# Make the predictions based off of the optimum ridge coefficients
ridge.test.predictions = as.matrix(cbind(const = 1, test.predictors)) %*% best.ridge.coeffs
ridge.MSE = mean((test$solubility - ridge.test.predictions)^2)
```

The test MSE for the ridge regression model is `r round(ridge.MSE, 3)`. The optimal 1SE $\lambda$ found by cross-validation was `r best.ridge.lambda`.

# Part c): LASSO

For LASSO, I'll use a similar range of potential $\lambda$'s as Ridge to cross-validate against.

```{r part-3 }
lasso.model = cv.glmnet(predictors, response, 
                        alpha = 1, lambda = lambdas, type.measure = "mse")

best.lasso.lambda = lasso.model$lambda.1se
best.lasso.coeffs = predict(lasso.model, s = best.lasso.lambda, type = "coefficients")

# Make the predictions based off of the optimum ridge coefficients
lasso.test.predictions = as.matrix(cbind(const = 1, test.predictors)) %*% best.lasso.coeffs
lasso.MSE = mean((test$solubility - lasso.test.predictions)^2)
```

The test MSE for the LASSO model was `r round(lasso.MSE, 3)`. The optimal 1SE $\lambda$ found by cross-validation was `r best.lasso.lambda`. The number of non-zero parameters kept by LASSO was `r sum(best.lasso.coeffs != 0) - 1`.

# Part d) PCR

```{r part-4 }
pcr.model = pcr(solubility ~ ., data = train, 
                scale = TRUE, validation = "CV", ncomp = 226)
```

```{r}
validationplot(pcr.model, val.type = "MSEP")
```

```{r part-4-mse }
optimal.comps = selectNcomp(pcr.model, "onesigma")
pcr.preds = predict(pcr.model, test, ncomp = optimal.comps)
pcr.mse = mean((pcr.preds - test$solubility)^2)
```

The best number of components to use was found using `selectNcomp()`, using the value within one sigma of the absolute optimum. This value will be our M. The test error from PCR was `r pcr.mse`.

# Part e) Summary

The performance of all 4 models is summarized below:

```{r summary }
summary = tibble(
  `Model` = c("Least Squares", "Ridge", "LASSO", "PCR"),
  `Number non-intercept parameters` = c(length(ls.model$coefficients) - 1,
                                        length(best.ridge.coeffs) - 1,
                                        sum(best.lasso.coeffs != 0) - 1, 
                                        optimal.comps),
  `Lambda/M` = c("N/A", round(best.ridge.lambda, 3), 
                 round(best.lasso.lambda, 3), optimal.comps),
  `Test MSE` = c(ls.MSE, ridge.MSE, lasso.MSE, pcr.mse)
)

knitr::kable(summary)
```

Out of the four models, the LASSO model performed the best in terms of having the smallest test MSE, given the parameters. Although the ridge model has similar test MSE, it suffers from poor model interpretability. The worst performing model was the saturated, simple least squares, even with the benefit of stepwise regression and cross-validation.
