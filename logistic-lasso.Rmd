---
title: "CD Logistic LASSO"
author: "Christian Pascual"
date: "2/28/2019"
output: html_document
---

Goal: Build a logistic-LASSO model to select features, and impletement the pathwise coordinatewise optimization algorithm to obtain a path of solutions with a sequence of descending λ’s.

```{r setup, message = FALSE }
library(tidyverse)
library(glmnet)
library(modelr)
library(caret)
```

## Important Equations

The goal of this part of the optimizaiton project is to implement logistic-LASSO regression using coordinate descent. The following are important equations for this task:

Using the Taylor expansion around "current estimates" $(\beta_0, \mathbf{\beta_1})$, we get an approximation to the logistic log-likelihood:
$$f(\beta_0, \mathbf{\beta_1}) \approx l(\beta_0, \mathbf{\beta_1}) = -\frac{1}{2n} \sum^n_{i=1} \omega_i(z_i  -\beta_0 - \mathbf{x}^T_i\mathbf{\beta_1})^2 + C(\tilde{\beta_0}, \tilde{\mathbf{\beta_1}})$$

where we have the working response:
$$z_i = \tilde{\beta_0} + \mathbf{x}^T_i\tilde{\mathbf{\beta_1}} + \frac{y_i - \tilde{p_i}(x_i)}{\tilde{p_i}(x_i)(1 - \tilde{p_i}(x_i))}$$

the working weights:
$$\omega_i = \tilde{p_i}(x_i)(1 - \tilde{p_i}(x_i))$$
and the probability given the current parameters:
$$\tilde{p_i}(x_i) = \frac{e^{\tilde{\beta_0} + \mathbf{x}^T_i\mathbf{\beta_1}}}{1 + e^{\tilde{\beta_0} + \mathbf{x}^T_i\mathbf{\beta_1}}}$$

We thus want to minimize the following equation
$$\mathop{min}_{(\beta_0, \mathbf{\beta_1)}} \bigg( \frac{1}{2n} \sum^n_{i=1} \omega_i(z_i  -\beta_0 - \mathbf{x}^T_i\mathbf{\beta_1})^2  + \lambda \sum^p_{j=0}|\beta_j| \bigg)$$

In coordinate descent, we know that if each observation has some weight $\omega_i$ associated with it, the updated $\tilde{\beta_j}$ is given by a weighted version of the soft-thresholding function:
$$\tilde{\beta_j} = \frac{S(\sum_i\omega_ix_{i,j}(y_i - \tilde{y}_i^{(-j)}), \gamma)}{\sum\omega_ix^2_{i,j}}$$

## Standardize the data for the regularization

```{r data, message = FALSE }
standardize = function(col) {
  mean = mean(col)
  stdev = sd(col)
  return((col - mean)/stdev)
}

# just standardize the covariates
standardized.data = read.csv(file = "breast-cancer-1.csv") %>% 
  dplyr::select(radius_mean:fractal_dimension_worst) %>% 
  map_df(.x = ., standardize)

# add back in the response and ids
data = cbind(read.csv(file = "breast-cancer-1.csv") %>% dplyr::select(diagnosis), standardized.data) %>% 
  mutate(diagnosis = ifelse(diagnosis == "M", 1, 0))
```

```{r helper-functions }
soft.threshold = function(beta, gamma) {
  new.beta = beta
    if (abs(beta) > gamma && beta > 0) {
      new.beta = beta - gamma
    } else if (abs(beta) > gamma && beta < 0) {
      new.beta = beta + gamma
    } else {
      new.beta = 0
    }
  return(new.beta)
}

calc.cur.p = function(intercept, data, betas) {
  # return n x 1 array of current probabilities evaluated with given betas
  return(
    exp(intercept * rep(1, nrow(data)) + data %*% betas) / (1 + exp(intercept * rep(1, nrow(data)) + data %*% betas))
  )
}

calc.working.resp = function(intercept, data, resp, betas, p) {
  # return n x 1 array of working responses evaluated with given betas
  return(
    intercept * rep(1, nrow(data)) + data %*% betas + (resp - p) / (p * (1 - p))
  )
}

calc.working.weights = function(p) {
  # return n x 1 array of working weights for the data
  return(p * (1 - p))
}

calc.obj = function(data, weights, w.resp, intercept, betas, lambda) {
  # return the objective function of data and current params
  return(
    log.lik = 1/(2 * nrow(data)) * 
      sum((weights * (w.resp - intercept * rep(1, nrow(data)) - data %*% betas))^2) + lambda * sum(abs(betas))
  )
}
```

```{r log-lasso-algo }
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 1000) {
  
  ### Parameters: #####################################################
  # X : design matrix                                                 #
  # y : response variable (should be binary)                          #
  # beta : starting beta coefficients to start from                   #
  # lambda : constraining parameter for LASSO penalization            #
  # tol : how precise should our convergence be                       #
  # maxiter : how many iterations should be performed before stopping #
  #####################################################################
  
  # Turn the betas into their own matrix
  X = as.matrix(X)
  beta = as.matrix(beta)
  
  # Initialize important parameters before starting the coordinate descent
  beta0 = 1/length(y) * sum(y - X %*% beta)
  p = calc.cur.p(intercept = beta0, data = X, betas = beta)
  z = calc.working.resp(intercept = beta0, data = X, resp = y, 
                        betas = beta, p = p)
  omega = calc.working.weights(p)
  obj = calc.obj(data = X, weights = omega, w.resp = z,
                 intercept = beta0, betas = beta, lambda = lambda)

  # Initialize the row for tracking each of these parameters
  path = c(iter = 0, intercept = beta0, beta, obj = obj)
    
    for (j in 1:maxiter) {
      
      prev.beta = beta

      # Coordinate descent
      for (k in 1:length(beta)) {
        r = y - (X %*% beta) + (X[,k] * beta[k])
        threshold.val = sum(omega * X[,k] * r)
        beta[k] = (soft.threshold(threshold.val, gamma = lambda)) / sum(omega * X[,k]^2)  
      }
      
      # With new betas, recalculate the working parameters
      beta0 = mean(y) - sum(colMeans(X) * beta)
      p = calc.cur.p(intercept = beta0, data = X, betas = beta)
      z = calc.working.resp(intercept = beta0, data = X, resp = y, 
                        betas = beta, p = p)
      omega = calc.working.weights(p)
      obj = calc.obj(data = X, weights = omega, w.resp = z,
                     intercept = beta0, betas = beta, lambda = lambda)

      # Append it to tracking matrix
      path = rbind(path, c(iter = j, intercept = beta0, beta, obj = obj))
      
      # Break the loop if the diff between likelihoods is below tolerance
      if (
        norm(prev.beta - beta, "F") < tol
        ) { break } 
    } 
  
  return(list(
    path = as.tibble(path),
    coefficients = c(beta0, beta))
    )
  }
```

# Make sure that the algorithm works

```{r param-setup }
X = data %>% dplyr::select(radius_mean:fractal_dimension_worst) 
y = data$diagnosis

ls.beta = lm(diagnosis ~ ., data = data)
coeffs = as.matrix(ls.beta$coefficients[2:length(ls.beta$coefficients)])
log.fit = glm(diagnosis ~ ., family = binomial(link = "logit"), data = data,
              control = list(maxit = 50))
log.coeffs = log.fit$coefficients

## PROBLEM: some covariates are nearly perfectly correlated with each other, so 
## what should we consider to be the full model if we take some of these out?

lambda = 0.12
```


```{r cancer-fit-test }
# initial guess will be what is produced by regular linear regression


cancer.CD = LogLASSO.CD(X = X, y = y, beta = coeffs, lambda = lambda, tol = 1e-4)
```

```{r lambda-optimization }
# Generate data to visualize how the coefficients change with the logistic LASSO
lambda0 = max(coeffs)
lambda.seq = exp(seq(-5, lambda0, length = 100))

coeff.path = NULL
for (l in 1:length(lambda.seq)) {
  fit = LogLASSO.CD(X = X, y = y, beta = coeffs, lambda = lambda.seq[l])
  coeff.path = rbind(coeff.path, c(lambda = lambda.seq[l], fit$coefficients))
  
  print(paste("Iter", l, "done", sep = " ")) # progress bar
}
```

```{r lambda-viz }
colnames(coeff.path) = c("lambda", paste("V", 1:31, sep = ""))
tidy.lambda = as.tibble(coeff.path) %>% 
  gather(., key = "coeff", value = "coeff_est", V1:V31) %>% 
  mutate(
    log.lambda = log(lambda)
  )

ggplot(data = tidy.lambda, aes(x = log.lambda, y = coeff_est, color = coeff, group = coeff)) +
  geom_line(alpha = 0.5) +
  theme(legend.position = "right") +
  labs(
    title = "Log-LASSO Coefficient estimates as a function of log(lambda)",
    x = "log(lambda)",
    y = "Coefficient estimate"
  )
```

```{r 5-fold-cv }
set.seed(8160)
# lambdas to cross-validate against
beta = lm(diagnosis ~ ., data = data)
ls.coeffs = beta$coefficients[2:length(ls.beta$coefficients)]
lambda0 = max(ls.coeffs) # previously calculated as max of the LS betas
lambda.seq = exp(seq(-5, lambda0, length = 100))
avg.rmses = NULL

# Set up the datasets for cross-validation
folds = crossv_kfold(data, k = 5)
train.idx = folds[k,1][[1]][[toString(k)]]$idx
train = data[train.idx,]
train.X = train %>% dplyr::select(radius_mean:fractal_dimension_worst)
train.y = train$diagnosis
test = data[-train.idx,]
test.X = test %>% dplyr::select(radius_mean:fractal_dimension_worst)
test.y = test$diagnosis

for (l in lambda.seq) {
  rmses = NULL
  for (k in 1:nrow(folds)) {
    LogLASSO = LogLASSO.CD(X = train.X, y = train.y, beta = ls.coeffs, lambda = l)
    LL.coefs = LogLASSO$coefficients
    rmse = sum(sqrt((test.y - as.matrix(cbind(1 * rep(1, nrow(test.X)), test.X)) %*% LL.coefs)^2))
    rmses = cbind(rmses, rmse)
  }
  avg.rmses = cbind(avg.rmses, mean(rmses))
  print(paste("iter: lambda = ", l, "done"))
}
```

```{r}
plot.lambda = tibble(
  lambdas = lambda.seq,
  avg.test.MSE = c(avg.rmses)
)

ggplot(data = plot.lambda, aes(x = log(lambdas), y = avg.test.MSE)) +
  geom_line() +
  labs(
    title = "Average test MSE as a function of log(lambda)",
    x = "log(lambda)",
    y = "Average Test MSE"
  )
```

```{r optimal-lambda }
min.RMSE = min(plot.lambda$avg.test.MSE)
min.lambda = plot.lambda[which(plot.lambda$avg.test.MSE == min.RMSE),]$lambdas
```

After performing the 5-fold cross-validation, it seems that a $\lambda$ of `r min.lambda` minimizes the test RMSE.
