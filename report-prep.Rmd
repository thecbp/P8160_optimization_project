---
title: "Appendix: Code"
author: "Christian Pascual, Xinyi Lin, Junting Ren"
date: "3/7/2019"
output: pdf_document
---

```{r setup, eval = FALSE }
library(tidyverse)
library(glmnet)
library(modelr)
library(matrixcalc)
set.seed(8160)
```

# Data preparation
```{r data, message = FALSE, eval = FALSE }
standardize = function(col) {
  mean = mean(col)
  stdev = sd(col)
  return((col - mean)/stdev)
}

# just standardize the covariates
standardized.data = read.csv(file = "breast-cancer-1.csv") %>% 
  dplyr::select(radius_mean:fractal_dimension_worst) %>% 
  map_df(.x = ., standardize)

# add back in the response and ids
data = cbind(read.csv(file = "breast-cancer-1.csv") %>% dplyr::select(diagnosis), standardized.data) %>% 
  mutate(diagnosis = ifelse(diagnosis == "M", 1, 0))

needed.cols = c("diagnosis", "radius_mean", "texture_mean", "perimeter_mean",
                "smoothness_mean", "compactness_mean", "concavity_mean",
                "concave.points_mean", "fractal_dimension_mean",
                "symmetry_mean", "radius_se", "perimeter_se", "symmetry_se")
```

# Data setup
```{r, eval = FALSE}
# Christian's data preparation
cbp.X = data %>% 
  mutate(intercept = 1) %>% 
  dplyr::select(intercept, radius_mean, texture_mean, perimeter_mean, smoothness_mean, 
                compactness_mean, concavity_mean, concave.points_mean,
                fractal_dimension_mean, symmetry_mean, radius_se, 
                perimeter_se, symmetry_se) 

# Xinyi's data preparation
xl.x = data %>% 
  mutate(intercept = 1) %>% 
  dplyr::select(intercept, radius_mean, texture_mean, perimeter_mean,
                smoothness_mean, compactness_mean, concavity_mean,
                concave.points_mean, fractal_dimension_mean, symmetry_mean,
                radius_se, perimeter_se, symmetry_se) %>% 
  as.matrix(.)

cbp.y = data$diagnosis
xl.y = as.matrix(data$diagnosis)
```

# Functions needed for analysis
```{r helper-functions, eval = FALSE }
soft = function(beta, gamma) {
  ### Parameters:
  # beta : the original coefficient beta from a regression
  # gamma : the desired threshold to limit the betas at
  
  # returns a single adjusted value of the original beta
  
  # Get the magnitude of beta - gamma, if gamma is greater, just assign to zero
  return(sign(beta) * max(abs(beta) - gamma, 0))
}

calc.cur.p = function(data, betas) {
  ### Parameters:
  # intercept : the intercept term of the betas (scalar)
  # data : the associated data for each beta in betas (n x p matrix)
  # betas : all the non-intercept beta coefficients (p x 1 array)
  
  # return n x 1 array of current probabilities evaluated with given betas
  
  u = data %*% betas
  return(exp(u) / (1 + exp(u)))
}

calc.working.weights = function(p) {
  ### Parameters:
  # p : the working probabilities, calculated by calc.cur.p
  
  # return n x 1 array of working weights for the data
  return(p * (1 - p))
}

calc.working.resp = function(data, resp, betas) {
  ### Parameters: 
  # intercept : the intercept term of the betas (scalar)
  # data : the associated data for each beta in betas (n x p matrix)
  # resp : the response variable of the dataset (n x 1 array)
  # betas : all the non-intercept beta coefficients (p x 1 array)
  # p : the working probabilities, calculated by calc.cur.p
  
  # return n x 1 array of working responses evaluated with given betas
  u = data %*% betas
  p = calc.cur.p(data, betas)
  omega = calc.working.weights(p)
  return(u + ((resp - p) / omega))
}

calc.obj = function(betas, w, z, data, lambda) {
  ### Parameters:
  # intercept : the intercept term of the betas (scalar)
  # data : the associated data for each beta in betas (n x p matrix)
  # resp : the response variable of the dataset (n x 1 array)
  # betas : all the non-intercept beta coefficients (p x 1 array)

  # return the log-likelihood value for a logistic model
  
  u = data %*% betas
  return((1/(2 * nrow(data))) * sum(w * (z - u)^2) + lambda * sum(abs(betas)))
}
```

# Logistic LASSO implementation
```{r log-lasso-algo, eval = FALSE }
LogLASSO.CD = function(X, y, beta, lambda, tol = 1e-5, maxiter = 2500) {
  ### Parameters:
  # X : design matrix                                                 
  # y : response variable (should be binary)                          
  # beta : starting beta coefficients to start from                   
  # lambda : constraining parameter for LASSO penalization            
  # tol : how precise should our convergence be                       
  # maxiter : how many iterations should be performed before stopping 
  
  # return a list containing the matrix of the coefficients and the iteration matrix
  
  # Convert data into matrix for easier manipulation
  X = as.matrix(X)
  names(beta) = colnames(X) # Assign original covariate names to the betas
  
  # Initialize important parameters before starting the coordinate descent
  p = calc.cur.p(X, beta)
  omega = calc.working.weights(p)
  z = calc.working.resp(X, y, beta)
  obj = calc.obj(beta, omega, z, X, lambda)
  
  # Iteration setup
  j = 0
  diff.obj = Inf

  # Initialize iteration matrix
  path = c(iter = j, beta)
    
  while (j < maxiter && diff.obj > tol) {
    j = j + 1
      
    # Store the last betas for convergence check later
    prev.obj = obj

    # Coordinate descent through all of the betas 
    for (k in 1:length(beta)) {
      z.rm.j = X[,-k] %*% beta[-k]
      val = sum(omega * X[,k] * (z - z.rm.j))
      beta[k] = soft(val, lambda) / sum(omega * X[,k]^2)  
    }
    
    # Recalculate the working parameters
    p = calc.cur.p(X, beta)
    omega = calc.working.weights(p)
    z = calc.working.resp( X, y, beta)
    obj = calc.obj(beta, omega, z, X, lambda)
      
    # Convergence check calculation
    diff.obj = abs(prev.obj - obj)
    
    # Append it to tracking matrix
    path = rbind(path, c(iter = j, beta))
    } 
  
  return(list(
    path = as.tibble(path),
    coefficients = beta)
    )
  }
```

# Newton-Raphson implementation
```{r, eval = FALSE}
logisticstuff <- function(x, y, betavec) {
  u <- x %*% betavec
  expu <- exp(u)
  loglik = vector(mode = "numeric", nrow(x))
  for(i in 1:nrow(x))
    loglik[i] = y[i]*u[i] - log(1 + expu[i])
  loglik_value = sum(loglik)
  # Log-likelihood at betavec
  p <- expu / (1 + expu)
  # P(Y_i=1|x_i)
  grad = vector(mode = "numeric", length(betavec))
  #grad[1] = sum(y - p)
  for(i in 1:13)
    grad[i] = sum(t(x[,i])%*%(y - p))
  #Hess <- -t(x)%*%p%*%t(1-p)%*%x
  Hess = hess_cal(x, p)
  return(list(loglik = loglik_value, grad = grad, Hess = Hess))
}

hess_cal = function(x, p) {
  len = length(p)
  hess = matrix(0, ncol(x), ncol(x))
  for (i in 1:len) {
    unit = x[i,] %*% t(x[i,]) * p[i] *(1 - p[i])
    #unit = t(x[i,])%*%x[i,]*p[i]*(1-p[i])
    hess = hess + unit
  }
  return(-hess)
}

NewtonRaphson <- function(x, y, logisticstuff, start, tol = 1e-5, maxiter = 200) {
  i <- 0
  cur <- start
  stuff <- logisticstuff(x, y, cur)
  res = c(0, cur)
  #res <- c(0, stuff$loglik, cur)
  prevloglik <- -Inf      # To make sure it iterates
  #while(i < maxiter && abs(stuff$loglik - prevloglik) > tol && stuff$loglik > -Inf)
  while (i < maxiter && abs(stuff$loglik - prevloglik) > tol) {
    i <- i + 1
    prevloglik <- stuff$loglik
    print(prevloglik)
    prev <- cur
    cur <- prev - solve(stuff$Hess) %*% stuff$grad
    stuff <- logisticstuff(x, y, cur)        # log-lik, gradient, Hessian
    res = rbind(res, c(i, cur))
    #res <- rbind(res, c(i, stuff$loglik, cur))
    # Add current values to results matrix
    }
  return(res)
}
modified <- function(x, y, logisticstuff, start, tol=1e-5, maxiter = 200){ 
  i <- 0 
  cur <- start 
  beta_len <- length(start)
  stuff <- logisticstuff(x, y, cur) 
  res = c(0, cur)
  #res <- c(0, stuff$loglik,cur)
  prevloglik <- -Inf # To make sure it iterates 
  while(i <= maxiter && abs(stuff$loglik - prevloglik) > tol)
  #while(i <= maxiter && abs(stuff$loglik - prevloglik) > tol && stuff$loglik > -Inf) 
    { i <- i + 1 
    prevloglik <- stuff$loglik 
    prev <- cur 
    lambda = 0
    while (is.negative.definite(stuff$Hess-lambda*diag(beta_len)) == FALSE) {
      lambda = lambda + 1
    }
    cur <- prev - solve(stuff$Hess-lambda*diag(beta_len)) %*% stuff$grad
    #cur <- prev + (diag(beta_len)/10)%*%(stuff$grad)
    #cur = prev + t(stuff$grad)%*%(stuff$grad)
    stuff <- logisticstuff(x, y, cur) # log-lik, gradient, Hessian
    res = rbind(res, c(i, cur))
    #res <- rbind(res, c(i, stuff$loglik, cur))
    }  
  return(res)
  }
```

# Visualizations

## betas vs lambda
```{r lambda-optimization, eval = FALSE }
# Generate data to visualize how the coefficients change with the logistic LASSO
lambda.seq = exp(seq(-10, 10, length = 300))
coeffs = rep(0.001, 13)
coeff.path = NULL

for (l in 1:length(lambda.seq)) {
  fit = LogLASSO.CD(X = cbp.X, y = cbp.y, beta = coeffs, lambda = lambda.seq[l])
  coeffs = fit$coefficients
  coeff.path = rbind(coeff.path, c(lambda = lambda.seq[l], coeffs))
  print(paste("Iter", l, "done", sep = " ")) # progress bar
}

tidy.lambda = as.tibble(coeff.path) %>% 
  gather(., key = "coeff", value = "coeff_est", intercept:symmetry_se) %>% 
  mutate(log.lambda = log(lambda))

ggplot(data = tidy.lambda, aes(x = log.lambda, y = coeff_est, color = coeff, group = coeff)) +
  geom_line(alpha = 0.5) +
  labs(
    title = "Log-LASSO Coefficient estimates as a function of log(lambda)",
    x = "log(lambda)",
    y = "Coefficient estimate"
  ) +
  theme(legend.position = "bottom", plot.title = element_text(hjust = 0.5))
```

## Cross-validation to find the best lambda
```{r 5-fold-cv, eval = FALSE }
avg.fold.mses = NULL
avg.fold.se = NULL
coeffs = rep(0.001, 13)

# Set up the datasets for cross-validation
folds = crossv_kfold(cbp.X, k = 5)
i = 0

for (l in lambda.seq) {
  # Reset the storage of the fold MSEs
  fold.mses = NULL
  i = i + 1
  for (k in 1:nrow(folds)) {
    
    # Grab the specific training indexes
    train.idx = folds[k,1][[1]][[toString(k)]]$idx
    
    # Split up the data into the training and test datasets
    train.X = cbp.X[train.idx,]
    test.X = cbp.X[-train.idx,]
    train.y = cbp.y[train.idx]
    test.y = cbp.y[-train.idx]

    # Perform the logistic-LASSO
    LogLASSO = LogLASSO.CD(X = train.X, y = train.y, beta = coeffs, lambda = l)
    coeffs = LogLASSO$coefficients
    
    # Calculate the test MSE for the fold
    fold.mse = mean((test.y - as.matrix(test.X) %*% coeffs)^2)
    fold.mses = c(fold.mses, fold.mse)
  }
  avg.fold.mses = cbind(avg.fold.mses, mean(fold.mses))
  avg.fold.se = cbind(avg.fold.se, sqrt(var(fold.mses)/k))
  print(paste("iter:", i, "done"))
}

# Set up plot for checking lambda against the average fold MSE 
plot.lambda = tibble(
  lambdas = lambda.seq,
  avg.fold.MSE = c(avg.fold.mses),
  avg.fold.SE = c(avg.fold.se),
  log.lambdas = log(lambda.seq)
)

min.MSE = min(plot.lambda$avg.fold.MSE)
min.lambda = plot.lambda[which(plot.lambda$avg.fold.MSE == min.MSE),]$log.lambdas

ggplot(data = plot.lambda, aes(x = log.lambdas, y = avg.fold.MSE)) +
  geom_line() +
  geom_errorbar(aes(ymin = avg.fold.MSE - avg.fold.SE, ymax = avg.fold.MSE + avg.fold.SE), color = "gray50") + 
  geom_vline(xintercept = min.lambda, alpha = 0.5, color = "red") +
  labs(
    title = "Average test MSE as a function of log(lambda)",
    x = "log(lambda)",
    y = "Average Test MSE"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))
```

# Tabulation

# Assemble all models
```{r, eval = FALSE}
# Fit all three models
start.betas = rep(0.001,13)
NR.fit = modified(xl.x, xl.y, logisticstuff, start.betas)
LL.fit = LogLASSO.CD(X = cbp.X, y = cbp.y, beta = start.betas, lambda = exp(min.lambda))
```

```{r, eval = FALSE}
coeff.table = tibble(
  `Coefficient` = c("Intercept", "Mean radius", "Mean texture", "Mean perimeter", 
                    "Mean smoothness", "Mean compactness", "Mean concavity",
                    "Mean concave points", "Mean fractal dimension", "Mean symmetry",
                    "Standard error radius", "Standard error perimeter", "Standard error symmetry"),
  `Newton-Raphson` = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)],
  `Logistic-LASSO` = LL.fit$coefficients
)
knitr::kable(coeff.table)
```

# Evaluating predictive ability

```{r, eval = FALSE}
coeffs = rep(0.001, 13)
NR.coefs = NR.fit[nrow(NR.fit), 2:ncol(NR.fit)]
# Set up the datasets for cross-validation
folds = crossv_kfold(cbp.X, k = 10)

NR.mses = NULL
LL.mses = NULL
for (k in 1:nrow(folds)) {
  
  # Grab the specific training indexes
    train.idx = folds[k,1][[1]][[toString(k)]]$idx
    
    # Split up the data into the training and test datasets
    train.X = cbp.X[train.idx,]
    test.X = cbp.X[-train.idx,]
    train.y = cbp.y[train.idx]
    test.y = cbp.y[-train.idx]
    
  LogLASSO = LogLASSO.CD(X = train.X, y = train.y, beta = coeffs, lambda = exp(min.lambda))
  coeffs = LogLASSO$coefficients
    
  LL.mse = mean((test.y - as.matrix(test.X) %*% coeffs)^2)
  LL.mses = cbind(LL.mses, LL.mse)
  
  NR.mse = mean((test.y - as.matrix(test.X) %*% NR.coefs)^2)
  NR.mses = cbind(NR.mses, NR.mse)
  }
```

```{r, eval = FALSE}
tidy.mses = tibble(
  `Logistic LASSO` = c(LL.mses),
  `Newton-Raphson` = c(NR.mses)
) %>% 
  gather(., key = "model", value = "test.MSE", `Logistic LASSO`:`Newton-Raphson`)

tidy.mses %>% 
  ggplot(data = ., aes(x = test.MSE, color = model, fill = model)) +
  geom_histogram(bins = 200) + 
  theme(legend.position = "bottom") +
  labs(
    title = "Distribution of test MSE in 10-fold cross validation by model",
    x = "Test MSE",
    y = "Density"
  ) + 
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
# Sanity checks for seeing if the algorithms are correct

f = formula(diagnosis ~ radius_mean + texture_mean + perimeter_mean + smoothness_mean + compactness_mean + concavity_mean + concave.points_mean + fractal_dimension_mean + symmetry_mean + radius_se + perimeter_se + symmetry_se)

X = cbp.X %>% as.matrix(.)
Y = as.matrix(cbp.y)
# Coefficients from regular glm
logit.fit = glm(f, family = binomial(link = "logit"), data = data)

# Coefficienst from cross-validated logistic-LASSO
log.lasso = cv.glmnet(X, Y, alpha = 1, family = "binomial", nfold = 5, type.measure = "mse")
best.log.lasso = glmnet(X, Y, alpha = 1, family = "binomial", lambda = log.lasso$lambda.min)
best.glmnet.coeffs = coef(best.log.lasso)
```

